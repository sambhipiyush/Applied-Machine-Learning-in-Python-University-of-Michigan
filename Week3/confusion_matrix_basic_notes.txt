	Confusion Matrix
------------------------
########################
------------------------

		   	PREDICTED
		  |	 N   P
		----------------
		N | TN   FP
ACTUAL 	  |
		P | FN	 TP
		  |

Type 1 Error -> FP
Type 2 Error -> FN

TN -> True Negative
FP -> False Positive -> T1 Error
FN -> False Negative -> T2 Error
TP -> True Positive

In TN/FP/FN/TP --> T and F refers to classification error (T for Correct Prediction i.e., when actual Negative is predicted as Negative or actual Positive is predicted as Positive and vice versa), and N and P refers to the Predicted class under which prediction falls (N for Negative and P for Positive).


-- What fraction of all isntances is the classifier's prediction correct( for either negative or positive class )
Accuracy = (TP + TN) / (TP + TN + FP + FN)


-- What fraction of all instances is the classifier's prediction incorrect
Classification Error = (FN + FP) / (TP + TN + FP + FN) == 1 - Accuracy


-- What fraction of all positive instances does the classifier correctly identify as positive?
Recall = True Positive Rate (TPR) = Sensitivity = Probability of Detection = TP / (FN + TP)  --> related to Actual part


-- What fraction of positive predictions are correct?
Precision = TP / (TP + FP)  --> related to Predicted part

-- What fraction of all negative instances does the classifier incorrectly identify as positive?
False Positive Rate (FPR) = Specificity = FP / (TN + FP)


Precision-Recall Trade-off
----------------------------

1. High Precision - Lower Recall
	-- FN instances increase in this case

2. Low Precision - High Recall
	-- FP instances increase in this case


Recall Oriented ML Tasks
	- Search and information extraction in legal discovery
	- Tumor detection
	- Often paired with a human experts	to filter out false positives (FP)

Precision Oriented ML Tasks
	- Search engine ranking, query suggestion
	- Document classification



F1-Score
----------
Combining precision and recall into a single number

Based on Harmonic mean of Precision and Recall.

F1-score = 2 * ( (Precision * Recall) / (Precision + Recall) ) = ( 2 * TP ) / ( 2 * TP + FN + FP )


Fβ = (1 + β^2) * ( (Precision * Recall) / ( (β^2 * Precision) + Recall) ) = ( (1 + β^2) * TP ) / ( (1 + β^2) * TP + β * FN + FP )

β allows adjustment of the metric to control the emphasis on recall vs precision
	- Precision-oriented users: β = 0.5 ( as we want FP to hurt performance more than FN )
	- Recall-oriented users: β = 2 ( as we want FN to hurt performance more than FP )
	- Precision-Recall weighed equally when:  β = 1 ( F1 score special case that we just saw that weights precision and recall equally )


-----------------------------------------------------------------
-----------------------------------------------------------------

1. Precision-Recall Curve
	- X-axis --> Precision
	- Y-axis --> Recall

	- "Steepness" of P-R Curves is important:
		-- Maximize Precision
		-- while maximizing Recall


2. ROC Curve (Receiver Operating Characteristic)
	- X-axis --> FP Rate
	- Y-axis --> TP Rate

	- "Steepness" of ROC curves is important:
		-- Maximize the TP rate
		-- while minimizing the FP rate


##########################################################################################
Medium Article: Precision vs Recall
-------------------------------------------

https://towardsdatascience.com/precision-vs-recall-386cf9f89488 

##########################################################################################


-----------------------------------------------------------------
-----------------------------------------------------------------

Macro Average vs Micro Average
----------------------------------

1. If the classes have about the same number of instances, macro and micro-average will be about the same
2. If some classes are much larger (have more instances) than others and you want to: 
	- Weight your metric toward the largest ones, use micro-averaging.
	- Weight your metric towards the smallest classes, use macro-averaging.
3. If the micro-average is much lower than the macro-average, then examine the larger classes for poor metric performance.
4. If the macro-average is much lower than the micro-average, then you should examine the smaller classes to see why they have poor metric performance.

